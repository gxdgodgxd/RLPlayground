{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---reset---\n",
      "<class 'numpy.ndarray'>\n",
      "(4,)\n",
      "{}\n",
      "---step---\n",
      "<class 'numpy.ndarray'>\n",
      "(4,)\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "# env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "def printObj(tag, obs):\n",
    "    print(f\"---{tag}---\")\n",
    "    print(type(obs))\n",
    "    print(obs.shape)\n",
    "\n",
    "def debugReset(env):\n",
    "    state, info = env.reset()\n",
    "    printObj(\"reset\", state)\n",
    "    print(info)\n",
    "\n",
    "def debugStep(env):\n",
    "    next_state, reward, done, trunc, info = env.step(action=0)\n",
    "    printObj(\"step\", next_state)\n",
    "    print(info)\n",
    "\n",
    "debugReset(env)\n",
    "debugStep(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            frame = self.env.render()\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return frame, total_reward, done, trunk, info\n",
    "\n",
    "    def reset(self, **kargs):\n",
    "        state, info = self.env.reset()\n",
    "        frame = self.env.render()\n",
    "        return frame, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, info = env.reset()\n",
    "# next_state, reward, done, trunc, info = env.step(action=0)\n",
    "\n",
    "# import cv2\n",
    "\n",
    "# for i in range(len(next_state)):\n",
    "#     frame = next_state[i]\n",
    "#     print(type(frame))\n",
    "#     print(frame.shape)\n",
    "#     cv2.imshow('Frame', frame.numpy())\n",
    "#     if cv2.waitKey(500) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # 关闭所有 OpenCV 窗口\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device=self.device)\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "        Inputs:\n",
    "        state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
    "        Outputs:\n",
    "        ``action_idx`` (``int``): An integer representing which action Mario will perform\n",
    "        \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):  # subclassing for continuity\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (``LazyFrame``),\n",
    "        next_state (``LazyFrame``),\n",
    "        action (``int``),\n",
    "        reward (``float``),\n",
    "        done(``bool``))\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(state)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        action = torch.tensor([action])\n",
    "        reward = torch.tensor([reward])\n",
    "        done = torch.tensor([done])\n",
    "\n",
    "        # self.memory.append((state, next_state, action, reward, done,))\n",
    "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
    "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    \"\"\"mini CNN structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = self.__build_cnn(c, output_dim)\n",
    "\n",
    "        self.target = self.__build_cnn(c, output_dim)\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "    def __build_cnn(self, c, output_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n",
      "\n",
      "Episode 0 - Step 3 - Epsilon 0.9999992500001874 - Mean Reward 9.0 - Mean Length 3.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.278 - Time 2025-03-03T17:58:36\n",
      "Episode 20 - Step 87 - Epsilon 0.9999782502338077 - Mean Reward 15.095 - Mean Length 4.143 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 2.196 - Time 2025-03-03T17:58:38\n",
      "Episode 40 - Step 164 - Epsilon 0.9999590008353578 - Mean Reward 14.39 - Mean Length 4.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.596 - Time 2025-03-03T17:58:40\n",
      "Episode 60 - Step 253 - Epsilon 0.9999367519923243 - Mean Reward 14.885 - Mean Length 4.148 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.413 - Time 2025-03-03T17:58:41\n",
      "Episode 80 - Step 331 - Epsilon 0.9999172534133322 - Mean Reward 14.531 - Mean Length 4.086 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.254 - Time 2025-03-03T17:58:42\n",
      "Episode 100 - Step 420 - Epsilon 0.9998950054991684 - Mean Reward 14.85 - Mean Length 4.17 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.426 - Time 2025-03-03T17:58:44\n",
      "Episode 120 - Step 485 - Epsilon 0.9998787573353123 - Mean Reward 13.93 - Mean Length 3.98 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.179 - Time 2025-03-03T17:58:45\n",
      "Episode 140 - Step 575 - Epsilon 0.9998562603135491 - Mean Reward 14.45 - Mean Length 4.11 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.531 - Time 2025-03-03T17:58:46\n",
      "Episode 160 - Step 659 - Epsilon 0.9998352635499218 - Mean Reward 14.38 - Mean Length 4.06 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.334 - Time 2025-03-03T17:58:48\n",
      "Episode 180 - Step 735 - Epsilon 0.9998162668580062 - Mean Reward 14.43 - Mean Length 4.04 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.317 - Time 2025-03-03T17:58:49\n",
      "Episode 200 - Step 820 - Epsilon 0.9997950209854148 - Mean Reward 14.29 - Mean Length 4.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.316 - Time 2025-03-03T17:58:50\n",
      "Episode 220 - Step 896 - Epsilon 0.9997760250581009 - Mean Reward 14.86 - Mean Length 4.11 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.206 - Time 2025-03-03T17:58:52\n",
      "Episode 240 - Step 968 - Epsilon 0.9997580292493605 - Mean Reward 14.1 - Mean Length 3.93 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.148 - Time 2025-03-03T17:58:53\n",
      "Episode 260 - Step 1059 - Epsilon 0.9997352850100658 - Mean Reward 14.23 - Mean Length 4.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.447 - Time 2025-03-03T17:58:54\n",
      "Episode 280 - Step 1143 - Epsilon 0.9997142907868937 - Mean Reward 14.42 - Mean Length 4.08 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.331 - Time 2025-03-03T17:58:56\n",
      "Episode 300 - Step 1234 - Epsilon 0.9996915475426376 - Mean Reward 14.66 - Mean Length 4.14 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.399 - Time 2025-03-03T17:58:57\n",
      "Episode 320 - Step 1338 - Epsilon 0.9996655558970418 - Mean Reward 15.72 - Mean Length 4.42 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.541 - Time 2025-03-03T17:58:58\n",
      "Episode 340 - Step 1428 - Epsilon 0.9996430636722579 - Mean Reward 16.46 - Mean Length 4.6 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.453 - Time 2025-03-03T17:59:00\n",
      "Episode 360 - Step 1494 - Epsilon 0.999626569695719 - Mean Reward 15.54 - Mean Length 4.35 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.932 - Time 2025-03-03T17:59:02\n",
      "Episode 380 - Step 1602 - Epsilon 0.9995995801393203 - Mean Reward 16.6 - Mean Length 4.59 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.882 - Time 2025-03-03T17:59:04\n",
      "Episode 400 - Step 1689 - Epsilon 0.9995778390821662 - Mean Reward 16.53 - Mean Length 4.55 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 2.062 - Time 2025-03-03T17:59:06\n",
      "Episode 420 - Step 1782 - Epsilon 0.9995545991646645 - Mean Reward 16.12 - Mean Length 4.44 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.721 - Time 2025-03-03T17:59:08\n",
      "Episode 440 - Step 1868 - Epsilon 0.9995331089691137 - Mean Reward 15.97 - Mean Length 4.4 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.676 - Time 2025-03-03T17:59:09\n",
      "Episode 460 - Step 1954 - Epsilon 0.9995116192355973 - Mean Reward 16.63 - Mean Length 4.6 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.674 - Time 2025-03-03T17:59:11\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 1, 8, 8], expected input[1, 4, 84, 84] to have 1 channels, but got 4 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Play the game!\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Run agent on the state\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mmario\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Agent performs action\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     next_state, reward, done, trunc, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[0;32mIn[48], line 37\u001b[0m, in \u001b[0;36mMario.act\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     35\u001b[0m     state \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m__array__() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m state\u001b[38;5;241m.\u001b[39m__array__()\n\u001b[1;32m     36\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m     action_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     action_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(action_values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# decrease exploration_rate\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/shims/versions/3.10.9/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/shims/versions/3.10.9/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 26\u001b[0m, in \u001b[0;36mMarioNet.forward\u001b[0;34m(self, input, model)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, model):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monline\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/shims/versions/3.10.9/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/shims/versions/3.10.9/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/shims/versions/3.10.9/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/shims/versions/3.10.9/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/shims/versions/3.10.9/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/shims/versions/3.10.9/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/shims/versions/3.10.9/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 8, 8], expected input[1, 4, 84, 84] to have 1 channels, but got 4 channels instead"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(1, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 4000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if (e % 20 == 0) or (e == episodes - 1):\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
