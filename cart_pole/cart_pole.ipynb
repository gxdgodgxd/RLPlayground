{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")  # 添加 render_mode=\"human\"\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "if os.path.exists('cart_pole_model.pth'):\n",
    "    print('loaded...')\n",
    "    policy_net.load_state_dict(torch.load('cart_pole_model.pth'))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[1;32m     33\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m target_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Compute V(s_{t+1}) for all next states.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Expected values of actions for non_final_next_states are computed based\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# on the \"older\" target_net; selecting their best reward with max(1).values\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# This is merged based on the mask, such that we'll have either the expected\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# state value or 0 in case the state was final.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m next_state_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(BATCH_SIZE, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     32\u001b[0m     next_state_values[non_final_mask] \u001b[38;5;241m=\u001b[39m target_net(non_final_next_states)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Compute the expected Q values\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/shims/versions/3.10.9/lib/python3.10/site-packages/torch/autograd/grad_mode.py:83\u001b[0m, in \u001b[0;36mno_grad.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        env.render()  # 添加这一行来渲染环境\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "OrderedDict([('layer1.weight', tensor([[-2.9628e-01, -4.4551e-01, -3.4243e-01, -4.4326e-01],\n",
      "        [-4.8560e-01,  1.5666e-01, -9.0720e-02, -2.2814e-01],\n",
      "        [-7.7540e-02, -4.3839e-01, -2.3526e-01,  3.9405e-01],\n",
      "        [ 6.0663e-01, -3.0964e-01, -2.3084e-01,  2.2046e-01],\n",
      "        [ 1.8824e-01,  1.2933e-01, -1.5140e-01,  9.4259e-02],\n",
      "        [-4.5509e-01,  5.8107e-02,  7.6821e-01,  4.5615e-01],\n",
      "        [ 3.2795e-01, -4.1728e-01,  1.7994e-01, -2.6640e-01],\n",
      "        [ 6.2724e-02,  3.3056e-01,  1.0025e-01, -1.4211e-01],\n",
      "        [ 2.1138e-01, -3.9414e-02, -2.3802e-01, -3.0691e-02],\n",
      "        [ 5.3817e-01,  1.8174e-01,  4.2960e-01,  2.7048e-01],\n",
      "        [-4.3940e-01, -2.9759e-01, -1.8268e-01, -4.1750e-01],\n",
      "        [ 1.6581e-01,  5.4729e-01,  4.2882e-01,  5.5193e-01],\n",
      "        [ 4.4103e-01, -3.1598e-01, -4.6012e-01, -2.9145e-01],\n",
      "        [-5.4524e-02, -1.0755e-01,  9.0717e-01,  5.1000e-01],\n",
      "        [ 6.6106e-02,  1.7421e-01,  8.3558e-01,  2.6362e-01],\n",
      "        [-6.8862e-01,  9.7873e-02,  4.7544e-01,  2.2263e-01],\n",
      "        [-5.4340e-01,  4.2250e-01,  3.5581e-02, -3.0902e-01],\n",
      "        [ 2.4790e-01, -2.1829e-01, -5.4531e-01, -4.4025e-01],\n",
      "        [-3.4091e-02, -4.3515e-01,  4.8355e-01,  5.2273e-01],\n",
      "        [ 3.5364e-01,  1.8226e-01, -6.4647e-01, -3.1452e-01],\n",
      "        [ 2.2425e-01,  2.3372e-01,  6.1248e-01,  4.2840e-01],\n",
      "        [-1.8129e-01, -3.1491e-01, -8.2441e-02, -1.0085e-02],\n",
      "        [-1.0501e-01,  1.4046e-01,  7.8270e-01, -1.2075e-01],\n",
      "        [-3.2621e-01,  4.0081e-01,  1.5337e-01, -3.4639e-01],\n",
      "        [-5.7772e-01,  4.3060e-01, -1.2159e-01, -1.6014e-01],\n",
      "        [-5.1631e-01,  2.0412e-01,  5.1759e-01, -2.7722e-01],\n",
      "        [ 2.2696e-01,  1.1056e-01,  3.8846e-02, -3.4114e-01],\n",
      "        [-5.2143e-01, -3.8934e-02,  5.3640e-01,  1.9956e-01],\n",
      "        [-5.3065e-02, -2.9683e-01, -5.2876e-01, -7.5622e-02],\n",
      "        [-1.8584e-01, -3.6881e-01, -3.0546e-01, -1.8443e-01],\n",
      "        [-2.4929e-01, -3.1047e-01,  2.6524e-01,  2.4782e-01],\n",
      "        [-2.5538e-01, -1.8088e-01,  4.0199e-01, -2.3612e-01],\n",
      "        [-5.2506e-02,  2.7858e-02, -7.2521e-01,  1.0475e-01],\n",
      "        [ 3.7357e-01,  1.8093e-01,  1.1035e-02,  1.7999e-01],\n",
      "        [-2.8820e-01,  4.6879e-02,  1.2320e-02, -3.3601e-01],\n",
      "        [-4.9904e-01,  2.6407e-01, -1.7273e-01, -1.6417e-01],\n",
      "        [-1.0109e-01, -1.1889e-01, -1.0231e-01,  5.0464e-02],\n",
      "        [-1.7970e-01,  2.4454e-03, -6.1815e-01,  4.0237e-01],\n",
      "        [ 1.1594e-01,  4.7248e-01, -3.8457e-01, -1.0742e-01],\n",
      "        [ 3.3554e-01, -3.0234e-01, -6.4610e-01, -3.7800e-01],\n",
      "        [-7.9708e-02, -1.5913e-01, -6.1001e-01, -4.0022e-01],\n",
      "        [-2.5959e-01, -1.1651e-01, -4.4878e-01, -2.5862e-01],\n",
      "        [-4.5044e-01,  2.4625e-01,  6.3282e-01, -1.5516e-01],\n",
      "        [-4.0205e-01, -1.4877e-01, -2.1759e-01, -9.5065e-02],\n",
      "        [ 3.6495e-02, -3.5597e-01,  6.4335e-01,  1.2567e-01],\n",
      "        [ 1.6763e-01,  2.5249e-01,  2.8531e-01,  2.1042e-01],\n",
      "        [-8.7241e-02,  2.6748e-01, -3.8895e-01,  2.4778e-01],\n",
      "        [-4.2088e-01,  1.9360e-01,  3.0182e-02,  1.7423e-01],\n",
      "        [ 1.5348e-01,  3.8774e-01, -3.2987e-01, -9.0641e-02],\n",
      "        [ 1.9932e-01,  2.1799e-01,  1.7095e-01,  2.8680e-01],\n",
      "        [ 2.5678e-01,  3.3315e-01, -5.8929e-01, -1.0024e-01],\n",
      "        [ 6.4631e-01,  1.3249e-01, -2.8137e-01, -2.5706e-01],\n",
      "        [ 4.0993e-01,  2.2212e-01, -1.5846e-01, -3.7132e-02],\n",
      "        [-2.7673e-01, -1.7663e-01,  2.7320e-02, -5.2034e-01],\n",
      "        [ 5.6940e-01, -3.0988e-01, -8.1775e-01, -3.3016e-01],\n",
      "        [-8.8641e-02,  3.0510e-01, -8.7037e-01, -4.5697e-01],\n",
      "        [-4.9322e-01,  8.0071e-02,  5.1682e-01, -1.1340e-01],\n",
      "        [-1.7780e-01,  2.7555e-01,  2.6863e-01,  1.2753e-01],\n",
      "        [-1.8631e-01,  2.9125e-02,  4.1532e-01, -6.2865e-02],\n",
      "        [ 1.4761e-01,  1.0387e-01,  8.7674e-01,  4.7072e-01],\n",
      "        [-2.1157e-01, -3.7286e-01, -9.6474e-02,  3.2062e-01],\n",
      "        [ 2.0013e-01,  3.2386e-01,  7.2884e-02, -3.4117e-01],\n",
      "        [-1.5577e-01,  2.4234e-01,  4.8954e-01, -2.0804e-01],\n",
      "        [ 4.1194e-01, -4.5201e-01, -7.9332e-02, -6.4271e-02],\n",
      "        [-2.7943e-01,  3.7254e-01,  3.5223e-01,  6.4294e-02],\n",
      "        [ 5.3434e-01, -2.4169e-01, -7.1491e-02,  4.2951e-01],\n",
      "        [-6.6616e-01, -3.1800e-01,  7.5581e-01, -4.0437e-01],\n",
      "        [-3.2280e-03, -1.5658e-01,  4.5223e-01, -2.5539e-01],\n",
      "        [-3.0670e-01,  3.3785e-01, -2.7738e-01,  6.3627e-03],\n",
      "        [ 1.3526e-02,  1.1596e-01, -2.3447e-01,  4.8483e-02],\n",
      "        [ 3.4328e-01, -4.7457e-01, -3.8467e-01, -4.2139e-02],\n",
      "        [ 5.6178e-01, -1.4520e-01, -4.9380e-01, -4.6520e-01],\n",
      "        [-3.1247e-01,  1.4856e-01, -6.3507e-02,  2.2483e-01],\n",
      "        [-5.5626e-01, -5.3824e-01,  1.7292e-02,  3.8692e-01],\n",
      "        [-1.9684e-01, -1.7470e-01, -5.6175e-03,  4.8492e-01],\n",
      "        [-2.2084e-01, -3.2325e-01,  6.5753e-01, -6.0685e-03],\n",
      "        [ 4.1160e-01, -3.6778e-02, -4.5170e-01,  5.4152e-02],\n",
      "        [-3.7712e-02, -7.5593e-03, -4.8260e-01, -2.8570e-01],\n",
      "        [-3.8169e-01, -3.2950e-01,  2.8157e-01, -7.3131e-02],\n",
      "        [ 8.2796e-02, -2.8495e-02,  2.1613e-01, -1.5134e-01],\n",
      "        [-5.3368e-02,  1.7441e-03, -8.3914e-01, -2.0866e-01],\n",
      "        [ 5.3035e-01, -3.5111e-01, -1.8183e-01,  3.4795e-01],\n",
      "        [ 5.1099e-01, -2.0172e-02,  2.2459e-01, -1.9123e-02],\n",
      "        [ 1.9925e-01, -1.0522e-01, -7.9180e-01, -2.6088e-02],\n",
      "        [ 1.7731e-01, -1.0303e-02, -2.0192e-01, -6.2066e-02],\n",
      "        [ 5.1695e-01, -2.6829e-01, -1.8808e-01, -4.9853e-01],\n",
      "        [ 5.4456e-01,  1.8230e-02, -1.1254e-01, -4.0835e-01],\n",
      "        [-2.6597e-02, -1.4660e-01,  1.8366e-01, -5.7340e-02],\n",
      "        [ 3.2431e-01,  1.3773e-01,  5.7939e-01,  3.6625e-01],\n",
      "        [-9.0715e-02,  9.7231e-03,  5.1490e-01, -2.7492e-01],\n",
      "        [ 1.7387e-01,  3.7424e-01,  5.4313e-01, -1.9185e-01],\n",
      "        [-3.8354e-01,  3.6760e-01, -3.7774e-02, -4.4867e-01],\n",
      "        [ 1.5835e-01, -3.5716e-01, -2.8060e-02,  2.2836e-01],\n",
      "        [-6.0146e-01, -2.4569e-01,  6.3019e-02,  4.4339e-01],\n",
      "        [ 5.6552e-01,  1.8677e-02, -6.7594e-02,  2.5153e-01],\n",
      "        [-4.9164e-01, -2.0590e-01,  1.6759e-01,  1.5225e-01],\n",
      "        [ 1.9119e-01, -3.5971e-01,  1.1930e-01, -7.5655e-02],\n",
      "        [ 2.7453e-01,  5.0714e-03, -5.2924e-01, -9.5406e-02],\n",
      "        [ 7.9917e-01, -2.2845e-01, -8.2313e-01, -2.9506e-01],\n",
      "        [ 5.1389e-01,  1.1485e-01, -3.6744e-01, -1.3613e-01],\n",
      "        [-3.8986e-01, -2.5484e-01,  2.0033e-01, -3.6736e-01],\n",
      "        [-2.4973e-01,  1.9074e-01,  3.7062e-01, -2.6039e-01],\n",
      "        [-2.1502e-01, -2.9505e-01, -8.1778e-02,  2.7551e-01],\n",
      "        [ 1.8324e-01,  1.9425e-01, -7.3670e-02, -2.4267e-01],\n",
      "        [-2.7379e-02,  1.8803e-02, -3.5359e-01, -4.8209e-01],\n",
      "        [-9.2474e-02, -4.8092e-01,  4.1200e-01, -2.4773e-01],\n",
      "        [ 2.7509e-01, -4.2203e-01,  3.5127e-01,  2.6854e-01],\n",
      "        [ 1.3395e-02, -4.1151e-01,  5.9748e-02, -6.8371e-02],\n",
      "        [ 1.0414e-01, -3.7158e-01,  4.3309e-02, -2.8035e-02],\n",
      "        [ 6.0535e-01, -7.5859e-04, -5.4128e-02, -1.8838e-01],\n",
      "        [-2.1495e-01, -4.0102e-01, -5.9180e-01,  3.9100e-01],\n",
      "        [ 4.3016e-01, -1.6926e-01, -6.5586e-01, -2.2273e-01],\n",
      "        [ 1.2940e-01, -4.7517e-01,  6.2517e-02, -2.4567e-01],\n",
      "        [ 7.0938e-01,  7.3910e-02, -1.3983e-01, -6.5179e-02],\n",
      "        [ 3.3715e-01,  2.8599e-02,  2.3528e-02,  2.1035e-01],\n",
      "        [ 7.3928e-02, -3.8233e-03, -2.0572e-02,  5.6581e-03],\n",
      "        [ 2.8772e-01, -2.4622e-02, -9.2870e-01, -2.8581e-01],\n",
      "        [-1.3066e-01,  4.3085e-01, -1.3372e-01,  2.2222e-01],\n",
      "        [-5.4361e-01, -1.4704e-01, -6.1757e-02, -5.2965e-02],\n",
      "        [ 4.8646e-01, -2.7861e-01, -5.5728e-01, -2.9102e-01],\n",
      "        [ 2.7053e-01, -2.9253e-01,  1.3568e-01,  3.9479e-01],\n",
      "        [-6.7926e-02,  1.9242e-02, -5.0375e-01,  1.7570e-01],\n",
      "        [ 1.5931e-01, -3.1687e-01,  6.2328e-01,  3.9646e-01],\n",
      "        [-4.7854e-01, -4.2080e-02,  7.2661e-01,  1.4869e-01],\n",
      "        [-7.9326e-02,  1.2647e-01,  1.3030e-01, -3.2020e-01],\n",
      "        [-2.8145e-02,  3.3004e-01,  3.2497e-01,  5.2339e-01],\n",
      "        [ 3.3993e-01,  3.5987e-02,  4.1409e-01, -1.8002e-01],\n",
      "        [ 1.6776e-01, -3.7122e-01, -3.2517e-01,  1.2389e-01]], device='mps:0')), ('layer1.bias', tensor([ 0.4153,  0.4866,  0.5853,  0.5608, -0.3130, -0.3880, -0.3332, -0.4091,\n",
      "         0.7290, -0.1472,  0.5810, -0.3893, -0.4047, -0.5823, -0.3016, -0.2827,\n",
      "         0.4932, -0.2749, -0.2471,  0.2483, -0.3740,  0.6245,  0.2389,  0.1868,\n",
      "         0.5839,  0.6220, -0.4118, -0.4612,  0.3001, -0.5081,  0.3283, -0.3439,\n",
      "         0.1795,  0.3791,  0.5714,  0.4872,  0.5667,  0.3837, -0.4079, -0.1868,\n",
      "        -0.3967,  0.5779, -0.2917, -0.4591, -0.5136,  0.7064,  0.3671,  0.3454,\n",
      "         0.2847,  0.5139, -0.4978, -0.1740, -0.0927, -0.1296, -0.3240, -0.4842,\n",
      "         0.2604,  0.3036, -0.4027, -0.5874,  0.6712,  0.2375,  0.2528,  0.4194,\n",
      "         0.0384,  0.3859,  0.3259,  0.2019,  0.6675, -0.3974,  0.3366, -0.4289,\n",
      "         0.6054, -0.0048, -0.3311, -0.4470, -0.4719, -0.4095, -0.4768, -0.3924,\n",
      "        -0.2568,  0.5661, -0.2523,  0.1322,  0.3612, -0.4450, -0.0274, -0.4682,\n",
      "        -0.5131,  0.7466,  0.4887,  0.4609,  0.6180, -0.4122,  0.4603,  0.1855,\n",
      "         0.6321,  0.5396, -0.3245, -0.2422,  0.6016,  0.4531,  0.5189, -0.4018,\n",
      "         0.4172,  0.6263, -0.4476, -0.3756,  0.7158, -0.3050,  0.1106, -0.2231,\n",
      "        -0.3033, -0.1843, -0.3914,  0.4487, -0.2609,  0.2879,  0.2062, -0.2112,\n",
      "         0.2579,  0.5325, -0.6190, -0.1916,  0.5163,  0.0682,  0.6023,  0.6761],\n",
      "       device='mps:0')), ('layer2.weight', tensor([[ 0.2056,  0.1485,  0.0539,  ..., -0.0626,  0.2477,  0.1583],\n",
      "        [ 0.1284,  0.1484,  0.0705,  ...,  0.0847,  0.1171,  0.0600],\n",
      "        [ 0.0077, -0.0232, -0.0362,  ...,  0.0235, -0.0617, -0.0154],\n",
      "        ...,\n",
      "        [ 0.1906,  0.0419,  0.0413,  ..., -0.0415,  0.1512,  0.2066],\n",
      "        [-0.0121, -0.0818,  0.0529,  ..., -0.0232, -0.0645, -0.0022],\n",
      "        [ 0.0853,  0.0686, -0.0308,  ..., -0.1662, -0.0837,  0.0209]],\n",
      "       device='mps:0')), ('layer2.bias', tensor([ 0.2348,  0.2481,  0.0675,  0.1474,  0.2333,  0.1106,  0.0963,  0.0085,\n",
      "        -0.0661, -0.0904,  0.1786,  0.0983,  0.1731,  0.0083,  0.0286,  0.1841,\n",
      "         0.1545,  0.1929,  0.0770, -0.0668,  0.2993,  0.1102, -0.0357,  0.0199,\n",
      "         0.1226,  0.0613, -0.0567,  0.0160,  0.1944,  0.1758,  0.0719,  0.0600,\n",
      "         0.0418,  0.1321, -0.0084,  0.2084, -0.0453,  0.2383,  0.1125,  0.0724,\n",
      "         0.0306, -0.0713,  0.2209,  0.2132,  0.2913,  0.0363,  0.2236,  0.2721,\n",
      "        -0.0089,  0.1922,  0.1706,  0.2217,  0.1701, -0.0980,  0.1788, -0.0990,\n",
      "         0.0872,  0.2561,  0.0340, -0.0618,  0.2348,  0.2356, -0.0243, -0.0848,\n",
      "         0.1875,  0.2386,  0.0833,  0.0772,  0.0258,  0.3093,  0.1373,  0.1240,\n",
      "         0.0287,  0.1107,  0.1404,  0.0241,  0.2470,  0.0746, -0.0588,  0.0543,\n",
      "        -0.0875,  0.0963,  0.1386,  0.0369,  0.1507, -0.0684,  0.1926, -0.0016,\n",
      "         0.0114,  0.1899,  0.0792,  0.0497, -0.0839,  0.0583,  0.1894, -0.1287,\n",
      "        -0.1064,  0.2170,  0.2726,  0.1500,  0.1007, -0.0445,  0.0194, -0.0407,\n",
      "         0.0372,  0.1333,  0.0777,  0.2830, -0.0337, -0.0387, -0.1195,  0.1042,\n",
      "         0.0768,  0.1206,  0.2045,  0.2115,  0.0854,  0.0937,  0.1860,  0.1554,\n",
      "         0.1631,  0.0408,  0.0412, -0.0429,  0.2396,  0.0756, -0.0703, -0.0151],\n",
      "       device='mps:0')), ('layer3.weight', tensor([[ 0.3005,  0.2739, -0.0378,  0.2754,  0.2066,  0.2204,  0.2995, -0.2815,\n",
      "         -0.5124, -0.2573,  0.2209,  0.2981,  0.2917, -0.0263,  0.2277,  0.2100,\n",
      "          0.2923,  0.2969, -0.0601,  0.0770,  0.2519,  0.3199, -0.1516, -0.3338,\n",
      "          0.2353, -0.2471, -0.0588, -0.1193,  0.2424,  0.2770, -0.1085, -0.0444,\n",
      "          0.1641,  0.1768, -0.3701,  0.3013, -0.2319,  0.3475,  0.1985,  0.3295,\n",
      "         -0.2956, -0.3508,  0.3072,  0.3121,  0.1857,  0.2482,  0.2086,  0.1652,\n",
      "         -0.1895,  0.1954,  0.2858,  0.1124,  0.2967, -0.3856,  0.3670, -0.3503,\n",
      "          0.3528,  0.3437, -0.2813, -0.0377,  0.2846,  0.2701, -0.5673,  0.0314,\n",
      "          0.3249,  0.1907,  0.1617, -0.0558, -0.3601,  0.2479,  0.2497,  0.3557,\n",
      "          0.3174,  0.2624,  0.2578,  0.0695,  0.3278, -0.0651, -0.2645, -0.0728,\n",
      "         -0.1287,  0.3578,  0.2998,  0.2696,  0.3111, -0.0604,  0.3277,  0.1080,\n",
      "         -0.5069,  0.2277,  0.3159,  0.0029, -0.0453,  0.3200,  0.1934, -0.5150,\n",
      "          0.0388,  0.1481,  0.2539,  0.2521,  0.1167, -0.0598, -0.3149, -0.0212,\n",
      "          0.0363,  0.1713, -0.0847,  0.2437, -0.0504,  0.0301,  0.1911,  0.2966,\n",
      "          0.2603,  0.2672,  0.2841,  0.2364,  0.3296,  0.2648,  0.2184,  0.0826,\n",
      "          0.2969, -0.3425,  0.3086, -0.1162,  0.2901,  0.2523, -0.0631, -0.0557],\n",
      "        [ 0.2311,  0.1085,  0.0331,  0.2392,  0.2355,  0.3039,  0.2180,  0.0563,\n",
      "         -0.2909, -0.0036,  0.3069,  0.2199,  0.0878, -0.0617, -0.3917,  0.2527,\n",
      "          0.2342,  0.2836,  0.0459, -0.0146,  0.2514,  0.3582, -0.4809, -0.0108,\n",
      "          0.2477,  0.0571,  0.0061, -0.3851,  0.3513,  0.1893,  0.1312,  0.0806,\n",
      "         -0.3032,  0.2566, -0.3228,  0.2910,  0.0602,  0.3841,  0.2933,  0.3432,\n",
      "          0.2315, -0.4187,  0.2342,  0.2206,  0.2710, -0.1880,  0.2517,  0.2952,\n",
      "         -0.1925,  0.2828,  0.3468,  0.2497,  0.2528, -0.4967,  0.2767, -0.1948,\n",
      "          0.3527,  0.2668, -0.0010, -0.0226,  0.2222,  0.3371, -0.3549, -0.0157,\n",
      "          0.2892,  0.2855, -0.2353, -0.0447, -0.1291,  0.2837,  0.1505,  0.3208,\n",
      "          0.2228, -0.1552,  0.2511, -0.2421,  0.3138,  0.0224, -0.0129, -0.0041,\n",
      "         -0.4524, -0.0403,  0.3076, -0.1010,  0.0387,  0.0522,  0.2656, -0.3618,\n",
      "         -0.4010,  0.2868,  0.2381,  0.0621, -0.0710,  0.3778,  0.3095, -0.3513,\n",
      "         -0.2733,  0.2599,  0.2450,  0.2424,  0.2859, -0.4750, -0.1453, -0.0031,\n",
      "          0.0733,  0.3192,  0.0071,  0.1525,  0.0846, -0.0172, -0.3295,  0.3165,\n",
      "          0.2721,  0.2467,  0.1628,  0.2705,  0.2473,  0.2544,  0.3726,  0.2541,\n",
      "          0.3358, -0.1247,  0.2568,  0.1846,  0.3021,  0.3617,  0.0139, -0.4523]],\n",
      "       device='mps:0')), ('layer3.bias', tensor([0.1050, 0.1144], device='mps:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(len(episode_durations))\n",
    "print(policy_net.state_dict())\n",
    "\n",
    "# for name, param in policy_net.named_parameters():\n",
    "#     print(f\"参数名称: {name}\")\n",
    "#     print(f\"参数形状: {param.shape}\")\n",
    "#     print(f\"参数值: {param.data}\")\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), 'cart_pole_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
